{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Is March\n",
    "by Chancellor Tang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, my goal was to see if it were possible to create a model that could predict the mens NCAA March Madness tournament. To do this, I retrieved 2 datasets. ncaam.csv, which has statistics for teams from tournament paticipants from the past five tournaments (2015-2019) With a column labeled \"POSTSEASON\", which stated how far each team went in the tournament. cbb21.csv hold team statistics from the 2021 season.\n",
    "\n",
    "Link: https://www.kaggle.com/andrewsundberg/college-basketball-dataset?select=cbb.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncaam = pd.read_csv('data/cbb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncaam21 = pd.read_csv('data/test/cbb21t.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ncaam22 = pd.read_csv('data/test/cbb22T.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncaam23 = pd.read_csv('data/test/cbb23t.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncaam24 = pd.read_csv('data/test/cbb24t.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleansing**\n",
    "\n",
    "The first step I took to cleanse was to drop all the schools not in the tournament out of the 2021 DataFrame. To do this, I created a condition to filter out any team with less than a 17 seed because 17 seeds do not exist in this tournament (yet). To make it easier for a later process, I also rearranged the columns so that the numeric values were grouped together. From there, for each DataFrame. I filled all null values with 0 and changed the seed number to an int. After restructuring the data, I divided the past tournaments DataFrame into their respective years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncaam21 = ncaam21[ncaam21['SEED'] < 17] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def power_conf(df):\n",
    "    test_list = []\n",
    "    for x in df.CONF:\n",
    "        if x in [\"B10\", \"B12\", \"SEC\", \"P12\", \"BE\", \"ACC\"]:\n",
    "            test_list.append(1)\n",
    "        else:\n",
    "            test_list.append(0)\n",
    "    return test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['TEAM',\n",
    " 'CONF',\n",
    " 'POSTSEASON',\n",
    " 'G',\n",
    " 'W',\n",
    " \"WIN_PER\",\n",
    " 'ADJOE',\n",
    " 'ADJDE',\n",
    " 'BARTHAG',\n",
    " 'EFG_O',\n",
    " 'EFG_D',\n",
    " 'TOR',\n",
    " 'TORD',\n",
    " 'ORB',\n",
    " 'DRB',\n",
    " 'FTR',\n",
    " 'FTRD',\n",
    " '2P_O',\n",
    " '2P_D',\n",
    " '3P_O',\n",
    " '3P_D',\n",
    " 'ADJ_T',\n",
    " 'WAB',\n",
    " 'SEED',\n",
    " \"POWER\",\n",
    " 'YEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_ncaa_df(df, year = None):\n",
    "    df = df.fillna(0)\n",
    "    df.SEED = df.SEED.astype(int) \n",
    "    df[\"WIN_PER\"] = df[\"W\"]/df[\"G\"]\n",
    "    df[\"POWER\"] = power_conf(df)\n",
    "    df = df.reindex(columns=new_columns)\n",
    "    if year is not None:\n",
    "        df[\"YEAR\"] = year\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncaam = format_ncaa_df(ncaam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ncaam21 = format_ncaa_df(ncaam21, 2021)\n",
    "ncaam22 = format_ncaa_df(ncaam22, 2022)\n",
    "ncaam23 = format_ncaa_df(ncaam23, 2023)\n",
    "ncaam24 = format_ncaa_df(ncaam24, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ncaam21 = ncaam[ncaam['YEAR']==2021]\n",
    "ncaam19 = ncaam[ncaam['YEAR']==2019]\n",
    "ncaam18 = ncaam[ncaam['YEAR']==2018]\n",
    "ncaam17 = ncaam[ncaam['YEAR']==2017]\n",
    "ncaam16 = ncaam[ncaam['YEAR']==2016]\n",
    "ncaam15 = ncaam[ncaam['YEAR']==2015]\n",
    "ncaam14 = ncaam[ncaam['YEAR']==2014]\n",
    "ncaam13 = ncaam[ncaam['YEAR']==2013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAM</th>\n",
       "      <th>CONF</th>\n",
       "      <th>POSTSEASON</th>\n",
       "      <th>G</th>\n",
       "      <th>W</th>\n",
       "      <th>WIN_PER</th>\n",
       "      <th>ADJOE</th>\n",
       "      <th>ADJDE</th>\n",
       "      <th>BARTHAG</th>\n",
       "      <th>EFG_O</th>\n",
       "      <th>...</th>\n",
       "      <th>FTRD</th>\n",
       "      <th>2P_O</th>\n",
       "      <th>2P_D</th>\n",
       "      <th>3P_O</th>\n",
       "      <th>3P_D</th>\n",
       "      <th>ADJ_T</th>\n",
       "      <th>WAB</th>\n",
       "      <th>SEED</th>\n",
       "      <th>POWER</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>B10</td>\n",
       "      <td>2ND</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>121.5</td>\n",
       "      <td>93.7</td>\n",
       "      <td>0.9522</td>\n",
       "      <td>54.6</td>\n",
       "      <td>...</td>\n",
       "      <td>22.7</td>\n",
       "      <td>53.4</td>\n",
       "      <td>47.6</td>\n",
       "      <td>37.9</td>\n",
       "      <td>32.6</td>\n",
       "      <td>64.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Louisville</td>\n",
       "      <td>BE</td>\n",
       "      <td>Champions</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>115.9</td>\n",
       "      <td>84.5</td>\n",
       "      <td>0.9743</td>\n",
       "      <td>50.6</td>\n",
       "      <td>...</td>\n",
       "      <td>34.9</td>\n",
       "      <td>50.8</td>\n",
       "      <td>43.4</td>\n",
       "      <td>33.3</td>\n",
       "      <td>31.8</td>\n",
       "      <td>67.1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Ohio St.</td>\n",
       "      <td>B10</td>\n",
       "      <td>E8</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>113.6</td>\n",
       "      <td>89.4</td>\n",
       "      <td>0.9406</td>\n",
       "      <td>50.6</td>\n",
       "      <td>...</td>\n",
       "      <td>29.5</td>\n",
       "      <td>49.4</td>\n",
       "      <td>43.6</td>\n",
       "      <td>35.6</td>\n",
       "      <td>32.4</td>\n",
       "      <td>65.3</td>\n",
       "      <td>7.2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Duke</td>\n",
       "      <td>ACC</td>\n",
       "      <td>E8</td>\n",
       "      <td>36</td>\n",
       "      <td>30</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>118.4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>0.9507</td>\n",
       "      <td>53.9</td>\n",
       "      <td>...</td>\n",
       "      <td>32.7</td>\n",
       "      <td>50.8</td>\n",
       "      <td>46.2</td>\n",
       "      <td>39.9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Marquette</td>\n",
       "      <td>BE</td>\n",
       "      <td>E8</td>\n",
       "      <td>35</td>\n",
       "      <td>26</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>113.0</td>\n",
       "      <td>93.2</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>49.6</td>\n",
       "      <td>...</td>\n",
       "      <td>31.7</td>\n",
       "      <td>51.6</td>\n",
       "      <td>44.9</td>\n",
       "      <td>29.6</td>\n",
       "      <td>32.3</td>\n",
       "      <td>64.6</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          TEAM CONF POSTSEASON   G   W   WIN_PER  ADJOE  ADJDE  BARTHAG  \\\n",
       "6     Michigan  B10        2ND  38  30  0.789474  121.5   93.7   0.9522   \n",
       "13  Louisville   BE  Champions  40  35  0.875000  115.9   84.5   0.9743   \n",
       "38    Ohio St.  B10         E8  37  29  0.783784  113.6   89.4   0.9406   \n",
       "39        Duke  ACC         E8  36  30  0.833333  118.4   91.5   0.9507   \n",
       "40   Marquette   BE         E8  35  26  0.742857  113.0   93.2   0.9020   \n",
       "\n",
       "    EFG_O  ...  FTRD  2P_O  2P_D  3P_O  3P_D  ADJ_T  WAB  SEED  POWER  YEAR  \n",
       "6    54.6  ...  22.7  53.4  47.6  37.9  32.6   64.8  6.2     4      1  2013  \n",
       "13   50.6  ...  34.9  50.8  43.4  33.3  31.8   67.1  9.0     1      1  2013  \n",
       "38   50.6  ...  29.5  49.4  43.6  35.6  32.4   65.3  7.2     2      1  2013  \n",
       "39   53.9  ...  32.7  50.8  46.2  39.9  29.0   67.8  7.5     2      1  2013  \n",
       "40   49.6  ...  31.7  51.6  44.9  29.6  32.3   64.6  4.5     3      1  2013  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncaam13.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual Input**\n",
    "\n",
    "\n",
    "One of the drawbacks from this dataframe is that it does not tell you which leg of the bracket each team came from. This causes an issue with setting up matchups. To regroup the data, I had to manually create lists for each leg (4) of each tournament (6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/regions.json\", \"r\") as file:\n",
    "    regions = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the Dataframes**\n",
    "\n",
    "For creating each leg's dataframe, I created three functions. f(x) works like a case function. It looks up the value of the column \"POSTSEASON\" and returns a list of ones and zeros to act as dummy variables. The dummy(z) function user f(x) to create a dataframe of of all these dummy variables. Finally, create_df(x,y) creates a final dataframe where it merges (the region list with their year's respective nccam dataframe to import the data. This also uses the dummy(z) function to add on the dummy variables.\n",
    "\n",
    "To create the dataframe, I used a for loop that creates the dataframes based on their region and their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_assign(x):\n",
    "    if x['POSTSEASON'] == \"R64\" : return [1,0,0,0,0,0,0]\n",
    "    elif x['POSTSEASON'] == \"R32\" : return [1,1,0,0,0,0,0]\n",
    "    elif x['POSTSEASON'] == \"S16\" : return [1,1,1,0,0,0,0]\n",
    "    elif x['POSTSEASON'] == 'E8': return [1,1,1,1,0,0,0]\n",
    "    elif x['POSTSEASON'] == 'F4': return [1,1,1,1,1,0,0]\n",
    "    elif x['POSTSEASON'] == '2ND' or x['POSTSEASON'] == 'C2': return [1,1,1,1,1,1,0]\n",
    "    elif x['POSTSEASON'] == 'Champions': return [1,1,1,1,1,1,1]\n",
    "    else: return [0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(z):\n",
    "    a = []\n",
    "    for x in range(0,len(z)):\n",
    "        a.append(round_assign(z.iloc[x]))\n",
    "    a = pd.DataFrame(a)\n",
    "    a = a.rename(columns={0: \"R64\", 1: \"R32\", 2: \"S16\",3: \"E8\", 4:\"F4\",5:\"C2\",6:\"Champions\"})\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_df(x,y):\n",
    "    v = pd.DataFrame(x)\n",
    "    v = v.rename(columns={0: \"TEAM\"})\n",
    "    v = v.merge(y, on = 'TEAM', how='left')\n",
    "    v = v.join(dummy(v))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame()\n",
    "\n",
    "all_years = [13,14,15,16,17,18,19,21,22,23,24]\n",
    "legs = [\"east\",'south', 'midwest', 'west']\n",
    "for x in all_years:\n",
    "    for y in legs:\n",
    "        z = str( 2000 + x)\n",
    "        globals()[y + '%s' % x + '%s' %\"_df\"] = region_df(regions[z][y],  globals()['ncaam' + '%s' % x])\n",
    "        master_df = pd.concat([master_df,globals()[y + '%s' % x + '%s' %\"_df\"]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df['games_won'] = master_df[['R64', 'R32', 'S16', 'E8', 'F4', 'C2', 'Champions']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.to_csv(\"master_ncaam.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determining an outcome**\n",
    "\n",
    "The main issue regarding the matchups was how to create a dataframe that can be trained to predict the outcome to a game. While prediciting likeliness of what round each team would reach is easier, this would not give us the same head-to-head chaos that March Madness embodies. The best way to predict the outcome of each induvidual game is to create a dataframe that shows head-to-head matchups. \n",
    "\n",
    "To do this, I created a function diff(df,u) that takes a region dataframe (df) and creates the head-to-head matchups. This function repeats for the number of matchups in the region for that round (u). The function take the first team in the dataframe and the last team in the dataframe, which are the 1 and 16 seed, and works their way in. So in the next matchup, it would be the 2 seed agains the second to last seed (15), and the for loop would keep going until the last matchup (8 vs 9) is created by moving its way from the outside in. For each match set by the for loop, both are run through the get_upset_differences(x,y), which subtracts the numeric statistics of y (the lower seed) by x (the higher seed) to create a difference. This difference is returned as  list of differences which is then appended to a its own list (listDF) in the diff(df,u) function. Once all the matchup differences are created, the function creates a dataframe using listDF and returns that dataframe. The predictive model will us this format of dataframe (the differences) as its training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headers = list(ncaam.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_years = [13,14,15,16,17,18,19,21,22,23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_past = []\n",
    "for x in range(0,len(legs)):\n",
    "    for y in train_years:\n",
    "        df_past.append(globals()[legs[x] + '%s' % y + '%s' %\"_df\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_upset_differences(a,b):\n",
    "    \"\"\"\n",
    "    Takes two rows from one of the region databases and finds the difference between the two columns \n",
    "\n",
    "    a: higher seed\n",
    "    b: lower seed'\n",
    "\n",
    "    output: list of db difference rows\n",
    "    \"\"\"\n",
    "    \n",
    "    listA = []\n",
    "    for x in range(3,25):\n",
    "        diff = a.iloc[x] - b.iloc[x]\n",
    "        listA.append(diff)\n",
    "    return listA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_variable(df, nxt_round_num, rnd_name):\n",
    "    \"\"\"\n",
    "    This function used to get the target variable attributes from the correct column\n",
    "    The list is reversed because we want to see if the lower seed wins\n",
    "\n",
    "    df: dataframe used\n",
    "    nxt_round_num: number of teams in the next round\n",
    "    rnd_name: the column name of the dummy round variable of the next round\n",
    "\n",
    "    output: list of target variable attributes (will be its own column)\n",
    "    \"\"\"\n",
    "    \n",
    "    num = int(len(df)/2)\n",
    "    listB = list(df[rnd_name])\n",
    "    listB.reverse()\n",
    "    listB = listB[0:num]\n",
    "    return listB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_record(df, matchup_num, reseed = True):\n",
    "    \"\"\"\n",
    "    This function takes the dataframe, and creates a new dataset of differences that can be trained\n",
    "    \n",
    "    df: dataframe of the round being processed\n",
    "    matchup_num: the number of matchups\n",
    "    \"\"\"\n",
    "    \n",
    "    listDF = []\n",
    "    for y in range(0,matchup_num):\n",
    "        test_upsetH = df.iloc[y]\n",
    "        test_upsetL = df.iloc[-(y+1)]\n",
    "        listDF.append(get_upset_differences(test_upsetH,test_upsetL))\n",
    "    if reseed:\n",
    "        return pd.DataFrame(listDF, columns = df_headers[3:25]).sort_values(by = [\"SEED\"])\n",
    "    else:\n",
    "        return pd.DataFrame(listDF, columns = df_headers[3:25]).sort_values(by = [\"SEED\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAM</th>\n",
       "      <th>CONF</th>\n",
       "      <th>POSTSEASON</th>\n",
       "      <th>G</th>\n",
       "      <th>W</th>\n",
       "      <th>WIN_PER</th>\n",
       "      <th>ADJOE</th>\n",
       "      <th>ADJDE</th>\n",
       "      <th>BARTHAG</th>\n",
       "      <th>EFG_O</th>\n",
       "      <th>...</th>\n",
       "      <th>SEED</th>\n",
       "      <th>POWER</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>R64</th>\n",
       "      <th>R32</th>\n",
       "      <th>S16</th>\n",
       "      <th>E8</th>\n",
       "      <th>F4</th>\n",
       "      <th>C2</th>\n",
       "      <th>Champions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>ACC</td>\n",
       "      <td>S16</td>\n",
       "      <td>37</td>\n",
       "      <td>30</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>114.6</td>\n",
       "      <td>89.5</td>\n",
       "      <td>0.9449</td>\n",
       "      <td>50.8</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Villanova</td>\n",
       "      <td>BE</td>\n",
       "      <td>R32</td>\n",
       "      <td>34</td>\n",
       "      <td>29</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>115.2</td>\n",
       "      <td>92.5</td>\n",
       "      <td>0.9260</td>\n",
       "      <td>53.6</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iowa St.</td>\n",
       "      <td>B12</td>\n",
       "      <td>S16</td>\n",
       "      <td>36</td>\n",
       "      <td>28</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>118.6</td>\n",
       "      <td>98.8</td>\n",
       "      <td>0.8903</td>\n",
       "      <td>54.2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michigan St.</td>\n",
       "      <td>B10</td>\n",
       "      <td>E8</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>119.6</td>\n",
       "      <td>95.3</td>\n",
       "      <td>0.9319</td>\n",
       "      <td>54.5</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cincinnati</td>\n",
       "      <td>Amer</td>\n",
       "      <td>R64</td>\n",
       "      <td>34</td>\n",
       "      <td>27</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>108.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>47.7</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>ACC</td>\n",
       "      <td>R32</td>\n",
       "      <td>34</td>\n",
       "      <td>24</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>113.4</td>\n",
       "      <td>94.7</td>\n",
       "      <td>0.8883</td>\n",
       "      <td>49.9</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>Amer</td>\n",
       "      <td>Champions</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>112.5</td>\n",
       "      <td>91.3</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>51.5</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Memphis</td>\n",
       "      <td>Amer</td>\n",
       "      <td>R32</td>\n",
       "      <td>33</td>\n",
       "      <td>23</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>112.6</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.8479</td>\n",
       "      <td>51.9</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>A10</td>\n",
       "      <td>R64</td>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>110.2</td>\n",
       "      <td>96.8</td>\n",
       "      <td>0.8151</td>\n",
       "      <td>51.5</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Saint Joseph's</td>\n",
       "      <td>A10</td>\n",
       "      <td>R64</td>\n",
       "      <td>34</td>\n",
       "      <td>24</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>113.8</td>\n",
       "      <td>97.5</td>\n",
       "      <td>0.8546</td>\n",
       "      <td>53.9</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>SEC</td>\n",
       "      <td>S16</td>\n",
       "      <td>36</td>\n",
       "      <td>23</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>118.1</td>\n",
       "      <td>93.4</td>\n",
       "      <td>0.9374</td>\n",
       "      <td>49.8</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Harvard</td>\n",
       "      <td>Ivy</td>\n",
       "      <td>R32</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>111.9</td>\n",
       "      <td>95.2</td>\n",
       "      <td>0.8641</td>\n",
       "      <td>52.1</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>CAA</td>\n",
       "      <td>R64</td>\n",
       "      <td>35</td>\n",
       "      <td>25</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>110.7</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>50.4</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Western Michigan</td>\n",
       "      <td>MAC</td>\n",
       "      <td>R64</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>104.7</td>\n",
       "      <td>102.5</td>\n",
       "      <td>0.5605</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Milwaukee</td>\n",
       "      <td>Horz</td>\n",
       "      <td>R64</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>104.3</td>\n",
       "      <td>103.9</td>\n",
       "      <td>0.5123</td>\n",
       "      <td>49.1</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Coastal Carolina</td>\n",
       "      <td>BSth</td>\n",
       "      <td>R64</td>\n",
       "      <td>31</td>\n",
       "      <td>18</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>98.3</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.3946</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 TEAM  CONF POSTSEASON   G   W   WIN_PER  ADJOE  ADJDE  \\\n",
       "0            Virginia   ACC        S16  37  30  0.810811  114.6   89.5   \n",
       "1           Villanova    BE        R32  34  29  0.852941  115.2   92.5   \n",
       "2            Iowa St.   B12        S16  36  28  0.777778  118.6   98.8   \n",
       "3        Michigan St.   B10         E8  38  29  0.763158  119.6   95.3   \n",
       "4          Cincinnati  Amer        R64  34  27  0.794118  108.7   91.5   \n",
       "5      North Carolina   ACC        R32  34  24  0.705882  113.4   94.7   \n",
       "6         Connecticut  Amer  Champions  40  32  0.800000  112.5   91.3   \n",
       "7             Memphis  Amer        R32  33  23  0.696970  112.6   97.0   \n",
       "8   George Washington   A10        R64  33  24  0.727273  110.2   96.8   \n",
       "9      Saint Joseph's   A10        R64  34  24  0.705882  113.8   97.5   \n",
       "10          Tennessee   SEC        S16  36  23  0.638889  118.1   93.4   \n",
       "11            Harvard   Ivy        R32  31  26  0.838710  111.9   95.2   \n",
       "12           Delaware   CAA        R64  35  25  0.714286  110.7  106.0   \n",
       "13   Western Michigan   MAC        R64  32  22  0.687500  104.7  102.5   \n",
       "14          Milwaukee  Horz        R64  34  20  0.588235  104.3  103.9   \n",
       "15   Coastal Carolina  BSth        R64  31  18  0.580645   98.3  102.0   \n",
       "\n",
       "    BARTHAG  EFG_O  ...  SEED  POWER  YEAR  R64  R32  S16  E8  F4  C2  \\\n",
       "0    0.9449   50.8  ...     1      1  2014    1    1    1   0   0   0   \n",
       "1    0.9260   53.6  ...     2      1  2014    1    1    0   0   0   0   \n",
       "2    0.8903   54.2  ...     3      1  2014    1    1    1   0   0   0   \n",
       "3    0.9319   54.5  ...     4      1  2014    1    1    1   1   0   0   \n",
       "4    0.8783   47.7  ...     5      0  2014    1    0    0   0   0   0   \n",
       "5    0.8883   49.9  ...     6      1  2014    1    1    0   0   0   0   \n",
       "6    0.9171   51.5  ...     7      0  2014    1    1    1   1   1   1   \n",
       "7    0.8479   51.9  ...     8      0  2014    1    1    0   0   0   0   \n",
       "8    0.8151   51.5  ...    11      0  2014    1    0    0   0   0   0   \n",
       "9    0.8546   53.9  ...    10      0  2014    1    0    0   0   0   0   \n",
       "10   0.9374   49.8  ...    11      1  2014    1    1    1   0   0   0   \n",
       "11   0.8641   52.1  ...    12      0  2014    1    1    0   0   0   0   \n",
       "12   0.6237   50.4  ...    13      0  2014    1    0    0   0   0   0   \n",
       "13   0.5605   52.0  ...    14      0  2014    1    0    0   0   0   0   \n",
       "14   0.5123   49.1  ...    15      0  2014    1    0    0   0   0   0   \n",
       "15   0.3946   48.0  ...    16      0  2014    1    0    0   0   0   0   \n",
       "\n",
       "    Champions  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "5           0  \n",
       "6           1  \n",
       "7           0  \n",
       "8           0  \n",
       "9           0  \n",
       "10          0  \n",
       "11          0  \n",
       "12          0  \n",
       "13          0  \n",
       "14          0  \n",
       "15          0  \n",
       "\n",
       "[16 rows x 33 columns]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_past64[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/3641443650.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    G   W   WIN_PER  ADJOE  ADJDE  BARTHAG  EFG_O  EFG_D  TOR  TORD  ...  \\\n",
       " 0   6  12  0.230166   16.3  -12.5   0.5503    2.8   -3.3 -3.7   0.1  ...   \n",
       " 1   0   9  0.264706   10.9  -11.4   0.4137    4.5   -2.8 -2.6   1.5  ...   \n",
       " 2   4   6  0.090278   13.9   -3.7   0.3298    2.2    0.1 -6.4  -2.6  ...   \n",
       " 3   3   4  0.048872    8.9  -10.7   0.3082    4.1   -3.6  4.0   0.9  ...   \n",
       " 4   3   1 -0.044592   -3.2   -3.7   0.0142   -4.4   -1.4 -0.2   1.3  ...   \n",
       " 5  -2   1  0.066993   -4.7    1.3  -0.0491    0.1    0.5  0.1   2.4  ...   \n",
       " 6   6   8  0.094118   -1.3   -6.2   0.0625   -2.4   -3.2 -1.4   4.3  ...   \n",
       " 7   0  -1 -0.030303    2.4    0.2   0.0328    0.4    1.2 -0.4   0.7  ...   \n",
       " \n",
       "    FTRD  2P_O  2P_D  3P_O  3P_D  ADJ_T   WAB  SEED  POWER  TRAIN  \n",
       " 0  -7.1   1.4  -4.6   4.3  -0.1   -6.3  15.9   -15      1      0  \n",
       " 1   2.1   4.7  -7.4   2.9   4.3   -0.2  14.4   -13      1      0  \n",
       " 2  -4.0   0.7  -1.8   3.2   2.6    4.6  10.4   -11      1      0  \n",
       " 3   2.3   2.6  -6.1   4.5   0.5   -6.1   8.3    -9      1      0  \n",
       " 4  -0.3  -3.1  -1.7  -5.1  -0.7   -2.0   3.1    -7      0      1  \n",
       " 5   9.9  -0.9   2.2   1.7  -2.8    7.7   3.8    -5      0      1  \n",
       " 6   5.1  -4.2  -3.8   0.8  -1.2   -1.1   2.6    -3      0      0  \n",
       " 7   0.4   2.5   1.8  -3.4  -0.2    2.3  -0.4    -3      0      0  \n",
       " \n",
       " [8 rows x 23 columns],\n",
       "             TEAM  CONF POSTSEASON   G   W   WIN_PER  ADJOE  ADJDE  BARTHAG  \\\n",
       " 0       Virginia   ACC        S16  37  30  0.810811  114.6   89.5   0.9449   \n",
       " 1      Villanova    BE        R32  34  29  0.852941  115.2   92.5   0.9260   \n",
       " 2       Iowa St.   B12        S16  36  28  0.777778  118.6   98.8   0.8903   \n",
       " 3   Michigan St.   B10         E8  38  29  0.763158  119.6   95.3   0.9319   \n",
       " 11       Harvard   Ivy        R32  31  26  0.838710  111.9   95.2   0.8641   \n",
       " 10     Tennessee   SEC        S16  36  23  0.638889  118.1   93.4   0.9374   \n",
       " 6    Connecticut  Amer  Champions  40  32  0.800000  112.5   91.3   0.9171   \n",
       " 7        Memphis  Amer        R32  33  23  0.696970  112.6   97.0   0.8479   \n",
       " \n",
       "     EFG_O  ...  FTRD  2P_O  2P_D  3P_O  3P_D  ADJ_T  WAB  SEED  POWER  YEAR  \n",
       " 0    50.8  ...  32.5  49.0  42.1  36.9  32.3   61.2  8.2     1      1  2014  \n",
       " 1    53.6  ...  38.6  53.8  43.0  35.6  35.3   67.6  8.3     2      1  2014  \n",
       " 2    54.2  ...  30.7  54.4  46.0  35.8  34.1   71.6  7.5     3      1  2014  \n",
       " 3    54.5  ...  40.3  52.1  43.5  39.2  33.2   66.2  6.9     4      1  2014  \n",
       " 11   52.1  ...  36.3  49.7  44.9  38.6  32.7   65.5  2.2    12      0  2014  \n",
       " 10   49.8  ...  31.5  50.6  44.5  31.9  34.5   63.1  0.4    11      1  2014  \n",
       " 6    51.5  ...  35.7  48.1  42.2  38.7  33.0   64.8  4.7     7      0  2014  \n",
       " 7    51.9  ...  33.8  52.7  49.2  33.0  31.0   69.5  2.8     8      0  2014  \n",
       " \n",
       " [8 rows x 26 columns])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creation(df_past64[1], 'R32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creation(df, next_rounds):\n",
    "    \"\"\"\n",
    "    This function takes the current round's dataframe and the next round string name to use the previous functions to build the training df\n",
    "\n",
    "    df: current round's dataframe\n",
    "    next_rounds\n",
    "    \"\"\"\n",
    "    \n",
    "    y = pd.DataFrame(columns = new_columns[3:25])\n",
    "    asd = []\n",
    "    nxt_round_num = int(len(df)/2)\n",
    "    upset= get_target_variable(df,nxt_round_num, next_rounds)\n",
    "    for x in range(0,nxt_round_num):\n",
    "        h = df.iloc[x]\n",
    "        l = df.iloc[(-x-1)]\n",
    "        if upset[x] == 1:\n",
    "            asd.append(l)\n",
    "        if upset[x] == 0:\n",
    "            asd.append(h)\n",
    "    next_df = pd.DataFrame(asd)\n",
    "    y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
    "    y[\"TRAIN\"] = upset\n",
    "    y.TRAIN = y.TRAIN.astype(int)\n",
    "    return y, next_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_past64 = []\n",
    "for x in range(0,len(legs)):\n",
    "    for y in train_years:\n",
    "        df_past64.append(globals()[legs[x] + '%s' % y + '%s' %\"_df\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train(a, next_round):\n",
    "    df = pd.DataFrame(columns = new_columns[3:25])\n",
    "    df_next = []\n",
    "    for x in a:\n",
    "        train, next_df = creation(x,next_round)\n",
    "        df = pd.concat([df,train], ignore_index = True)\n",
    "        df_next.append(next_df)\n",
    "    return df, df_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/2773149795.py:6: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df,train], ignore_index = True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1168495985.py:12: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train_master = pd.concat([train_master,holder[0]], ignore_index = True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1168495985.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train_w1 = pd.concat([train_w1,holder[0]], ignore_index = True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/2773149795.py:6: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df,train], ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/2773149795.py:6: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df,train], ignore_index = True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1168495985.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train_w2 = pd.concat([train_w2,holder[0]], ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/2773149795.py:6: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df,train], ignore_index = True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "round_name = [\"R32\", \"S16\", \"E8\", \"F4\"]\n",
    "train_master = pd.DataFrame(columns = new_columns[3:25])\n",
    "train_w1 = pd.DataFrame(columns = new_columns[3:25])\n",
    "train_w2 = pd.DataFrame(columns = new_columns[3:25])\n",
    "\n",
    "for x in range(0,len(round_name)):\n",
    "    a = 2**(6-x)\n",
    "    b = 2**(5-x)\n",
    "    holder = create_train(globals()[\"df_past\" + '%s' % a], round_name[x])\n",
    "    globals()[\"train\" + '%s' % a] = holder[0]\n",
    "    globals()[\"df_past\" + '%s' % b] = holder[1]\n",
    "    train_master = pd.concat([train_master,holder[0]], ignore_index = True)\n",
    "    print(\"end\")\n",
    "    if x <= 1:\n",
    "        train_w1 = pd.concat([train_w1,holder[0]], ignore_index = True)\n",
    "    else:\n",
    "        train_w2 = pd.concat([train_w2,holder[0]], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final 4 Training DF\n",
    "\n",
    "fin4 = df of all teams in the final 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pastF4 = []\n",
    "num = len(train_years)\n",
    "\n",
    "for p in range(num):\n",
    "    df_list = []\n",
    "    for i in range(4):\n",
    "        df_list.append(df_past4[(i * num) + p])\n",
    "    df = pd.concat(df_list)\n",
    "    df_pastF4.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfs = []\n",
    "for x in df_past:\n",
    "    y = x[x[\"F4\"]==1]\n",
    "    new_dfs.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin4 = pd.concat(new_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in train_years:\n",
    "    year = y + 2000\n",
    "    globals()[\"fin4_\" + '%s' % y] = fin4[fin4[\"YEAR\"]== year][\"TEAM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin4_13 = fin4_13.sort_values(ascending = True)\n",
    "\n",
    "fin4_14 = fin4_14.sort_values(ascending = True)\n",
    "fin4_14 = fin4_14.reindex([6,7,1,0])\n",
    "\n",
    "fin4_15 = fin4_15.sort_values(ascending = True)\n",
    "fin4_15 = fin4_15.reset_index(drop = True)\n",
    "fin4_15 = fin4_15.reindex([0,3,1,2])\n",
    "\n",
    "fin4_16 = fin4_16.sort_values(ascending = True)\n",
    "fin4_16 = fin4_16.reset_index(drop = True)\n",
    "fin4_16 = fin4_16.reindex([0,3,1,2])\n",
    "                          \n",
    "fin4_17 = fin4_17.sort_values(ascending = True)\n",
    "fin4_18 = fin4_18.sort_values(ascending = True)       \n",
    "fin4_19 = fin4_19.sort_values(ascending = True)\n",
    "\n",
    "fin4_21 = fin4_21.sort_values(ascending = True)\n",
    "fin4_21 = fin4_21.reset_index(drop = True)\n",
    "fin4_21 = fin4_21.reindex([0,3,1,2])\n",
    "\n",
    "fin4_21 = fin4_21.sort_values(ascending = True)\n",
    "fin4_21 = fin4_21.reset_index(drop = True)\n",
    "fin4_21 = fin4_21.reindex([0,3,1,2])\n",
    "\n",
    "fin4_22 = fin4_22.sort_values(ascending = True)\n",
    "fin4_22 = fin4_22.reset_index(drop = True)\n",
    "fin4_22 = fin4_22.reindex([0,3,1,2])\n",
    "\n",
    "fin4_23 = fin4_23.sort_values(ascending = True)\n",
    "fin4_23 = fin4_23.reset_index(drop = True)\n",
    "fin4_23 = fin4_23.reindex([0,3,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_past4 = []\n",
    "for x in train_years:\n",
    "    y = train_years.index(x)\n",
    "    df = pd.DataFrame(globals()[\"fin4_\" + '%s' % x ])\n",
    "    year = 2000 + x\n",
    "    df2 = df_pastF4[y]\n",
    "    df = df.merge(df2, left_on='TEAM', right_on='TEAM')\n",
    "    df_past4.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/2773149795.py:6: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df,train], ignore_index = True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/2773149795.py:6: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df,train], ignore_index = True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/1537820806.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y = pd.concat([y, create_training_record(df,nxt_round_num)], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "train4, df_past2 = create_train(df_past4, \"C2\")\n",
    "train2, df_past1 = create_train(df_past2, \"Champions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/2938079200.py:6: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train_ff = pd.concat([train_ff, x], ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "train_ff = pd.DataFrame(columns = new_columns[3:25])\n",
    "\n",
    "\n",
    "for x in [train4,train2]:\n",
    "    train_master = pd.concat([train_master,x], ignore_index = True)\n",
    "    train_ff = pd.concat([train_ff, x], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = train_master[train_master[\"SEED\"]<=0]\n",
    "train_pos = train_master[train_master[\"SEED\"]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = - train_pos\n",
    "train_pos[\"TRAIN\"] = train_pos[\"TRAIN\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_master = pd.concat([train_pos, train_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_master = train_master.reset_index(drop = True)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_master.drop(columns = [\"TRAIN\"]))\n",
    "\n",
    "def scale(df):\n",
    "    m = pd.DataFrame(scaler.transform(df), columns = df.columns)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_cutoff_low = -7\n",
    "seed_cutoff_high = -4\n",
    "big_upset = train_master[train_master[\"SEED\"] < seed_cutoff_low]\n",
    "little_upset = train_master[train_master[\"SEED\"] > seed_cutoff_high]\n",
    "competative = train_master[(train_master[\"SEED\"] <= seed_cutoff_high) & (train_master[\"SEED\"] >= seed_cutoff_low)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>W</th>\n",
       "      <th>WIN_PER</th>\n",
       "      <th>ADJOE</th>\n",
       "      <th>ADJDE</th>\n",
       "      <th>BARTHAG</th>\n",
       "      <th>EFG_O</th>\n",
       "      <th>EFG_D</th>\n",
       "      <th>TOR</th>\n",
       "      <th>TORD</th>\n",
       "      <th>...</th>\n",
       "      <th>FTRD</th>\n",
       "      <th>2P_O</th>\n",
       "      <th>2P_D</th>\n",
       "      <th>3P_O</th>\n",
       "      <th>3P_D</th>\n",
       "      <th>ADJ_T</th>\n",
       "      <th>WAB</th>\n",
       "      <th>SEED</th>\n",
       "      <th>POWER</th>\n",
       "      <th>TRAIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>-11.1</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.2</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>-3.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.1</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.011583</td>\n",
       "      <td>12.1</td>\n",
       "      <td>-6.7</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>-4.3</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>...</td>\n",
       "      <td>15.2</td>\n",
       "      <td>-6.7</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-4.7</td>\n",
       "      <td>7.1</td>\n",
       "      <td>-8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-6</td>\n",
       "      <td>-0.346154</td>\n",
       "      <td>10.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.1373</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>5.3</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>-9.1</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.5</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-5.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>-3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>11.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.4</td>\n",
       "      <td>7.6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>9.4</td>\n",
       "      <td>-8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.373992</td>\n",
       "      <td>6.9</td>\n",
       "      <td>-21.2</td>\n",
       "      <td>0.6295</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-9.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>-11.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>-4.3</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>16.1</td>\n",
       "      <td>-7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>11.6</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>-4.1</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-7.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>-0.0163</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-7.8</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.317139</td>\n",
       "      <td>2.2</td>\n",
       "      <td>-11.6</td>\n",
       "      <td>0.1015</td>\n",
       "      <td>4.7</td>\n",
       "      <td>-10.5</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>-12.5</td>\n",
       "      <td>3.7</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>6.1</td>\n",
       "      <td>-10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.076102</td>\n",
       "      <td>8.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.7</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>-3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>13.4</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0739</td>\n",
       "      <td>8.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>7.4</td>\n",
       "      <td>4.9</td>\n",
       "      <td>-5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      G   W   WIN_PER  ADJOE  ADJDE  BARTHAG  EFG_O  EFG_D  TOR  TORD  ...  \\\n",
       "0     6   0 -0.133333    8.0    6.8   0.0034   -0.8   -0.0 -2.4 -11.1  ...   \n",
       "1     2   1 -0.011583   12.1   -6.7   0.2720   -4.3   -0.0 -0.7   4.9  ...   \n",
       "2     3  -6 -0.346154   10.5    2.8   0.1373   -0.9    5.3 -2.8  -9.1  ...   \n",
       "3     0  11  0.379310   11.4    1.0   0.2182    5.1    3.9 -3.7   0.1  ...   \n",
       "4    -1  11  0.373992    6.9  -21.2   0.6295    3.5   -9.6  0.3  -3.0  ...   \n",
       "..   ..  ..       ...    ...    ...      ...    ...    ...  ...   ...  ...   \n",
       "595   2   2  0.005263   11.6    6.7   0.0058    1.3    2.6 -4.1  -4.8  ...   \n",
       "596   0   1  0.027027    0.2    2.1  -0.0163   -2.0    1.7 -2.2   2.5  ...   \n",
       "597   1  13  0.317139    2.2  -11.6   0.1015    4.7  -10.5 -2.2  -0.4  ...   \n",
       "598  -1   2  0.076102    8.2    4.7   0.0048    5.5    1.2 -2.8  -3.8  ...   \n",
       "599  -3   4  0.241379   13.4   -2.2   0.0739    8.9    2.1 -2.0   2.9  ...   \n",
       "\n",
       "     FTRD  2P_O  2P_D  3P_O  3P_D  ADJ_T   WAB  SEED  POWER  TRAIN  \n",
       "0   -14.2  -1.6  -3.2   0.5   4.1   -2.7   3.5    -8      1    0.0  \n",
       "1    15.2  -6.7  -3.3  -0.6   3.0   -4.7   7.1    -8      1    0.0  \n",
       "2   -14.5  -0.2   3.1  -1.0   6.0   -5.3   2.1    -3      1    1.0  \n",
       "3   -16.4   7.6   6.0  -0.1   0.7    0.1   9.4    -8      0    1.0  \n",
       "4    -7.9   2.9 -11.2   2.9  -4.3   -0.6  16.1    -7      0    1.0  \n",
       "..    ...   ...   ...   ...   ...    ...   ...   ...    ...    ...  \n",
       "595 -14.9   1.5   2.0   0.5   4.0   -7.4   2.3    -1      0    0.0  \n",
       "596   3.7   4.4   0.8  -7.8   2.5   -1.8  -1.3    -1      0    1.0  \n",
       "597  -6.5   4.3 -12.5   3.7  -4.4    3.1   6.1   -10     -1    0.0  \n",
       "598 -10.7   8.6   1.5  -0.2   0.7    4.5   0.0    -2     -1    1.0  \n",
       "599   1.1  11.9   4.6   1.7  -1.7    7.4   4.9    -5     -1    0.0  \n",
       "\n",
       "[600 rows x 23 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree as tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatStuff(df):\n",
    "    y = df[\"TRAIN\"]\n",
    "    x = scale(df.drop(columns = [\"TRAIN\"]))\n",
    "    return [x,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = formatStuff(train_master)\n",
    "train_big = formatStuff(big_upset)\n",
    "train_little = formatStuff(little_upset)\n",
    "train_comp = formatStuff(competative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31333333333333335\n",
      "600 \n",
      "\n",
      "0.1828793774319066\n",
      "257 \n",
      "\n",
      "0.40782122905027934\n",
      "179 \n",
      "\n",
      "0.4146341463414634\n",
      "164 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in [train_df, train_big, train_little, train_comp]:\n",
    "    print(sum(p[1])/len(p[0]))\n",
    "    print(len(p[0]), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.864\n",
      "Accuracy on training set: 0.966\n",
      "Accuracy on training set: 0.945\n"
     ]
    }
   ],
   "source": [
    "#MLP Classifier\n",
    "X = train_big[0]\n",
    "Y = list(train_big[1])\n",
    "\n",
    "x = train_little[0]\n",
    "y = list(train_little[1])\n",
    "\n",
    "train_x = train_comp[0]\n",
    "train_y = list(train_comp[1])\n",
    "\n",
    "iterations = 1000\n",
    "alpha = 3\n",
    " \n",
    "mlp_big = MLPClassifier(max_iter= iterations, alpha=alpha, random_state = 69)\n",
    "mlp_big.fit(X, Y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp_big.score(X,Y)))\n",
    "\n",
    "mlp_little = MLPClassifier(max_iter= iterations, alpha= alpha, random_state = 69)\n",
    "mlp_little.fit(x, y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp_little.score(x, y)))\n",
    "\n",
    "mlp_comp = MLPClassifier(max_iter= iterations, alpha= alpha, random_state = 69)\n",
    "mlp_comp.fit(train_x, train_y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp_comp.score(train_x, train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.911\n",
      "Accuracy on training set: 0.922\n",
      "Accuracy on training set: 0.872\n"
     ]
    }
   ],
   "source": [
    "#FOREST\n",
    "X = train_big[0]\n",
    "Y = list(train_big[1])\n",
    "\n",
    "x = train_little[0]\n",
    "y = list(train_little[1])\n",
    "\n",
    "train_x = train_comp[0]\n",
    "train_y = list(train_comp[1])\n",
    "\n",
    "est = 6\n",
    "depth = 5\n",
    " \n",
    "forest_big = RandomForestClassifier(n_estimators=est, max_depth = depth, random_state = 69)\n",
    "forest_big.fit(X, Y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    forest_big.score(X, Y)))\n",
    "\n",
    "forest_little = RandomForestClassifier(n_estimators=est, max_depth = depth, random_state = 69)\n",
    "forest_little.fit(x, y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    forest_little.score(x, y)))\n",
    "\n",
    "forest_comp = RandomForestClassifier(n_estimators=est, max_depth = depth, random_state = 69)\n",
    "forest_comp.fit(train_x, train_y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest_comp.score(train_x, train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.829\n",
      "Accuracy on training set: 0.905\n",
      "Accuracy on training set: 0.841\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "X = train_big[0]\n",
    "Y = list(train_big[1])\n",
    "\n",
    "x = train_little[0]\n",
    "y = list(train_little[1])\n",
    "\n",
    "train_x = train_comp[0]\n",
    "train_y = list(train_comp[1])\n",
    "\n",
    "svc_big = SVC(random_state = 69, C = 1)\n",
    "svc_big.fit(X, Y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(svc_big.score(X, Y)))\n",
    "\n",
    "svc_little = SVC(random_state = 69, C = 1)\n",
    "svc_little.fit(x, y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(svc_little.score(x, y)))\n",
    "\n",
    "svc_comp = SVC(random_state = 69, C = 1)\n",
    "svc_comp.fit(train_x, train_y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(svc_comp.score(train_x, train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.848\n",
      "Accuracy on training set: 0.849\n",
      "Accuracy on training set: 0.732\n"
     ]
    }
   ],
   "source": [
    "#Log Regressor\n",
    "X = train_big[0]\n",
    "Y = list(train_big[1])\n",
    "\n",
    "x = train_little[0]\n",
    "y = list(train_little[1])\n",
    "\n",
    "train_x = train_comp[0]\n",
    "train_y = list(train_comp[1])\n",
    "\n",
    "clf_big =  LogisticRegression(random_state=69, C =1)\n",
    "clf_big.fit(X, Y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(clf_big.score(X, Y)))\n",
    "\n",
    "clf_little = LogisticRegression(random_state=69, C = 5)\n",
    "clf_little.fit(x, y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(clf_little.score(x, y)))\n",
    "\n",
    "clf_comp = LogisticRegression(random_state=69, C = 20)\n",
    "clf_comp.fit(train_x, train_y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(clf_comp.score(train_x, train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.840\n",
      "Accuracy on training set: 0.782\n",
      "Accuracy on training set: 0.774\n"
     ]
    }
   ],
   "source": [
    "#K-Nearest\n",
    "X = train_big[0]\n",
    "Y = list(train_big[1])\n",
    "\n",
    "x = train_little[0]\n",
    "y = list(train_little[1])\n",
    "\n",
    "train_x = train_comp[0]\n",
    "train_y = list(train_comp[1])\n",
    "\n",
    "neighbors = 5\n",
    "\n",
    "knn_big = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "knn_big.fit(X, Y)\n",
    "knn_bscore = knn_big.score(X, Y)\n",
    "\n",
    "knn_little = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "knn_little.fit(x, y)\n",
    "knn_lscore = knn_little.score(x, y)\n",
    "\n",
    "knn_comp = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "knn_comp.fit(train_x, train_y)\n",
    "knn_cscore = knn_comp.score(train_x, train_y)\n",
    "\n",
    "knn_mean =  sum( [knn_bscore, knn_lscore, knn_cscore])/ len( [knn_bscore, knn_lscore, knn_cscore])\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(knn_bscore))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(knn_lscore))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(knn_cscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.743\n",
      "Accuracy on training set: 0.743\n",
      "Accuracy on training set: 0.671\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "X = train_big[0]\n",
    "Y = list(train_big[1])\n",
    "\n",
    "x = train_little[0]\n",
    "y = list(train_little[1])\n",
    "\n",
    "train_x = train_comp[0]\n",
    "train_y = list(train_comp[1])\n",
    "\n",
    "gnb_big = GaussianNB()\n",
    "gnb_big.fit(X, Y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gnb_big.score(X, Y)))\n",
    "\n",
    "gnb_little = GaussianNB()\n",
    "gnb_little.fit(x,y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gnb_little.score(x, y)))\n",
    "\n",
    "gnb_comp = GaussianNB()\n",
    "gnb_comp.fit(train_x, train_y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gnb_comp.score(train_x, train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.922\n",
      "Accuracy on training set: 0.894\n",
      "Accuracy on training set: 0.848\n"
     ]
    }
   ],
   "source": [
    "# Decision tree\n",
    "X = train_big[0]\n",
    "Y = list(train_big[1])\n",
    "\n",
    "x = train_little[0]\n",
    "y = list(train_little[1])\n",
    "\n",
    "train_x = train_comp[0]\n",
    "train_y = list(train_comp[1])\n",
    "\n",
    "tree_depth = 7\n",
    "\n",
    "DT_big = DecisionTreeClassifier(min_samples_leaf = 5, max_depth = tree_depth)\n",
    "DT_big.fit(X, Y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(DT_big.score(X, Y)))\n",
    "\n",
    "DT_little = DecisionTreeClassifier(min_samples_leaf = 5, max_depth = tree_depth)\n",
    "DT_little.fit(x, y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(DT_little.score(x,y)))\n",
    "\n",
    "DT_comp = DecisionTreeClassifier(min_samples_leaf = 5, max_depth = tree_depth)\n",
    "DT_comp.fit(train_x, train_y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(DT_comp.score(train_x, train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree as tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatStuff(df):\n",
    "    y = df[\"TRAIN\"]\n",
    "    x = scale(df.drop(columns = [\"TRAIN\"]))\n",
    "    return [x,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = formatStuff(train_master)\n",
    "train_big = formatStuff(big_upset)\n",
    "train_little = formatStuff(little_upset)\n",
    "train_comp = formatStuff(competative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31333333333333335\n",
      "600 \n",
      "\n",
      "0.1828793774319066\n",
      "257 \n",
      "\n",
      "0.40782122905027934\n",
      "179 \n",
      "\n",
      "0.4146341463414634\n",
      "164 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in [train_df, train_big, train_little, train_comp]:\n",
    "    print(sum(p[1])/len(p[0]))\n",
    "    print(len(p[0]), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.864\n",
      "Accuracy on training set: 0.966\n",
      "Accuracy on training set: 0.945\n"
     ]
    }
   ],
   "source": [
    "#MLP Classifier\n",
    "X = train_big[0]\n",
    "Y = list(train_big[1])\n",
    "\n",
    "x = train_little[0]\n",
    "y = list(train_little[1])\n",
    "\n",
    "train_x = train_comp[0]\n",
    "train_y = list(train_comp[1])\n",
    "\n",
    "iterations = 1000\n",
    "alpha = 3\n",
    " \n",
    "mlp_big = MLPClassifier(max_iter= iterations, alpha=alpha, random_state = 69)\n",
    "mlp_big.fit(X, Y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp_big.score(X,Y)))\n",
    "\n",
    "mlp_little = MLPClassifier(max_iter= iterations, alpha= alpha, random_state = 69)\n",
    "mlp_little.fit(x, y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp_little.score(x, y)))\n",
    "\n",
    "mlp_comp = MLPClassifier(max_iter= iterations, alpha= alpha, random_state = 69)\n",
    "mlp_comp.fit(train_x, train_y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp_comp.score(train_x, train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.911\n",
      "Accuracy on training set: 0.922\n",
      "Accuracy on training set: 0.872\n"
     ]
    }
   ],
   "source": [
    "#FOREST\n",
    "X = train_big[0]\n",
    "Y = list(train_big[1])\n",
    "\n",
    "x = train_little[0]\n",
    "y = list(train_little[1])\n",
    "\n",
    "train_x = train_comp[0]\n",
    "train_y = list(train_comp[1])\n",
    "\n",
    "est = 6\n",
    "depth = 5\n",
    " \n",
    "forest_big = RandomForestClassifier(n_estimators=est, max_depth = depth, random_state = 69)\n",
    "forest_big.fit(X, Y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    forest_big.score(X, Y)))\n",
    "\n",
    "forest_little = RandomForestClassifier(n_estimators=est, max_depth = depth, random_state = 69)\n",
    "forest_little.fit(x, y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    forest_little.score(x, y)))\n",
    "\n",
    "forest_comp = RandomForestClassifier(n_estimators=est, max_depth = depth, random_state = 69)\n",
    "forest_comp.fit(train_x, train_y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest_comp.score(train_x, train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.829\n",
      "Accuracy on training set: 0.905\n",
      "Accuracy on training set: 0.841\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "X = train_big[0]\n",
    "Y = list(train_big[1])\n",
    "\n",
    "x = train_little[0]\n",
    "y = list(train_little[1])\n",
    "\n",
    "train_x = train_comp[0]\n",
    "train_y = list(train_comp[1])\n",
    "\n",
    "svc_big = SVC(random_state = 69, C = 1)\n",
    "svc_big.fit(X, Y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(svc_big.score(X, Y)))\n",
    "\n",
    "svc_little = SVC(random_state = 69, C = 1)\n",
    "svc_little.fit(x, y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(svc_little.score(x, y)))\n",
    "\n",
    "svc_comp = SVC(random_state = 69, C = 1)\n",
    "svc_comp.fit(train_x, train_y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(svc_comp.score(train_x, train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.848\n",
      "Accuracy on training set: 0.849\n",
      "Accuracy on training set: 0.732\n"
     ]
    }
   ],
   "source": [
    "#Log Regressor\n",
    "X = train_big[0]\n",
    "Y = list(train_big[1])\n",
    "\n",
    "x = train_little[0]\n",
    "y = list(train_little[1])\n",
    "\n",
    "train_x = train_comp[0]\n",
    "train_y = list(train_comp[1])\n",
    "\n",
    "clf_big =  LogisticRegression(random_state=69, C =1)\n",
    "clf_big.fit(X, Y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(clf_big.score(X, Y)))\n",
    "\n",
    "clf_little = LogisticRegression(random_state=69, C = 5)\n",
    "clf_little.fit(x, y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(clf_little.score(x, y)))\n",
    "\n",
    "clf_comp = LogisticRegression(random_state=69, C = 20)\n",
    "clf_comp.fit(train_x, train_y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(clf_comp.score(train_x, train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.840\n",
      "Accuracy on training set: 0.782\n",
      "Accuracy on training set: 0.774\n"
     ]
    }
   ],
   "source": [
    "#K-Nearest\n",
    "X = train_big[0]\n",
    "Y = list(train_big[1])\n",
    "\n",
    "x = train_little[0]\n",
    "y = list(train_little[1])\n",
    "\n",
    "train_x = train_comp[0]\n",
    "train_y = list(train_comp[1])\n",
    "\n",
    "neighbors = 5\n",
    "\n",
    "knn_big = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "knn_big.fit(X, Y)\n",
    "knn_bscore = knn_big.score(X, Y)\n",
    "\n",
    "knn_little = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "knn_little.fit(x, y)\n",
    "knn_lscore = knn_little.score(x, y)\n",
    "\n",
    "knn_comp = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "knn_comp.fit(train_x, train_y)\n",
    "knn_cscore = knn_comp.score(train_x, train_y)\n",
    "\n",
    "knn_mean =  sum( [knn_bscore, knn_lscore, knn_cscore])/ len( [knn_bscore, knn_lscore, knn_cscore])\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(knn_bscore))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(knn_lscore))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(knn_cscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.743\n",
      "Accuracy on training set: 0.743\n",
      "Accuracy on training set: 0.671\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "X = train_big[0]\n",
    "Y = list(train_big[1])\n",
    "\n",
    "x = train_little[0]\n",
    "y = list(train_little[1])\n",
    "\n",
    "train_x = train_comp[0]\n",
    "train_y = list(train_comp[1])\n",
    "\n",
    "gnb_big = GaussianNB()\n",
    "gnb_big.fit(X, Y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gnb_big.score(X, Y)))\n",
    "\n",
    "gnb_little = GaussianNB()\n",
    "gnb_little.fit(x,y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gnb_little.score(x, y)))\n",
    "\n",
    "gnb_comp = GaussianNB()\n",
    "gnb_comp.fit(train_x, train_y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gnb_comp.score(train_x, train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.922\n",
      "Accuracy on training set: 0.894\n",
      "Accuracy on training set: 0.848\n"
     ]
    }
   ],
   "source": [
    "# Decision tree\n",
    "X = train_big[0]\n",
    "Y = list(train_big[1])\n",
    "\n",
    "x = train_little[0]\n",
    "y = list(train_little[1])\n",
    "\n",
    "train_x = train_comp[0]\n",
    "train_y = list(train_comp[1])\n",
    "\n",
    "tree_depth = 7\n",
    "\n",
    "DT_big = DecisionTreeClassifier(min_samples_leaf = 5, max_depth = tree_depth)\n",
    "DT_big.fit(X, Y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(DT_big.score(X, Y)))\n",
    "\n",
    "DT_little = DecisionTreeClassifier(min_samples_leaf = 5, max_depth = tree_depth)\n",
    "DT_little.fit(x, y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(DT_little.score(x,y)))\n",
    "\n",
    "DT_comp = DecisionTreeClassifier(min_samples_leaf = 5, max_depth = tree_depth)\n",
    "DT_comp.fit(train_x, train_y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(DT_comp.score(train_x, train_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the Predictions**\n",
    "\n",
    "To do this, I create dataframes for each round of the tournament. From there, I created three appendable variables: test_df as an x_test, y_pred as a y_test, and matchup list to keep track of which teams faced off as a visual check. The for loop takes all four regions and sets it equal to the round64 variable. Using the rounds list, which had all the dataframes up until the final four, the for loop would take each round and simulate each matchup by using the lists function to create those differences that were used in the x_train dataframe. Using the result from the lists function (holder), I used the predict function for each ML library to take the values and see if the game was an upset or not. For each game, it would print the matchup, and once the prediction was made, it would print the winner and append the winner to the wins list. Using the wins list, a dataframe would be created (rounds[x+1]). After creating the datframe, it would append the prediction to y_pred and the matchup to matchup_list. After all the rounds how gone, each dataframe created for the round is appended with data from the rounds[x+1].\n",
    "\n",
    "For each predictive model, it has to be input in the variable q because I have yet to create a loop that can run all these libraries concurrently. \"q\" is the type of model the prediction runs (SVC, MLP, regressor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "r64 = pd.DataFrame(columns = df_headers)\n",
    "r32 = pd.DataFrame(columns = df_headers)\n",
    "s16 = pd.DataFrame(columns = df_headers)\n",
    "e8 = pd.DataFrame(columns = df_headers)\n",
    "f4 = pd.DataFrame(columns = df_headers)\n",
    "c2= pd.DataFrame(columns = df_headers)\n",
    "winner= pd.DataFrame(columns = df_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINE TUNE [knn, DT, forest, mlp, clf, gnb, svc]\n",
    "b = \"knn\"\n",
    "l = \"gnb\"\n",
    "c = \"clf\"\n",
    "\n",
    "big = globals()[b + '%s' % \"_big\"]\n",
    "little = globals()[l + '%s' % \"_little\"]\n",
    "comp= globals()[c + '%s' % \"_comp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/4079259146.py:7: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  r64 = pd.concat([r64,x], ignore_index = True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/4079259146.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_df = pd.concat([test_df,holder], ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Connecticut  vs.  16 Stetson\n",
      "Winner: 1 Connecticut\n",
      "2 Iowa St.  vs.  15 North Dakota St.\n",
      "Winner: 2 Iowa St.\n",
      "3 Illinois  vs.  14 Morehead St.\n",
      "Winner: 3 Illinois\n",
      "4 Auburn  vs.  13 Yale\n",
      "Winner: 4 Auburn\n",
      "5 San Diego St.  vs.  12 UAB\n",
      "Winner: 5 San Diego St.\n",
      "6 BYU  vs.  11 Duquesne\n",
      "Winner: 6 BYU\n",
      "7 Washington St.  vs.  10 Drake\n",
      "Winner: 10 Drake\n",
      "8 Florida Atlantic  vs.  9 Northwestern\n",
      "Winner: 9 Northwestern\n",
      "1 Connecticut  vs.  9 Northwestern\n",
      "Winner: 1 Connecticut\n",
      "2 Iowa St.  vs.  10 Drake\n",
      "Winner: 2 Iowa St.\n",
      "3 Illinois  vs.  6 BYU\n",
      "Winner: 6 BYU\n",
      "4 Auburn  vs.  5 San Diego St.\n",
      "Winner: 4 Auburn\n",
      "1 Connecticut  vs.  4 Auburn\n",
      "Winner: 1 Connecticut\n",
      "2 Iowa St.  vs.  6 BYU\n",
      "Winner: 2 Iowa St.\n",
      "1 Connecticut  vs.  2 Iowa St.\n",
      "Winner: 1 Connecticut\n",
      "________________________________________\n",
      "1 Purdue  vs.  16 Grambling St.\n",
      "Winner: 1 Purdue\n",
      "2 Tennessee  vs.  15 Saint Peter's\n",
      "Winner: 2 Tennessee\n",
      "3 Creighton  vs.  14 Akron\n",
      "Winner: 3 Creighton\n",
      "4 Kansas  vs.  13 Samford\n",
      "Winner: 13 Samford\n",
      "5 Gonzaga  vs.  12 McNeese St.\n",
      "Winner: 5 Gonzaga\n",
      "6 South Carolina  vs.  11 Oregon\n",
      "Winner: 11 Oregon\n",
      "7 Texas  vs.  10 Colorado St.\n",
      "Winner: 7 Texas\n",
      "8 Utah St.  vs.  9 TCU\n",
      "Winner: 9 TCU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/4079259146.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  r32 = pd.concat([r32,rounds[1]], ignore_index = True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/4079259146.py:46: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  s16 = pd.concat([s16,rounds[2]], ignore_index = True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/4079259146.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  e8 = pd.concat([e8,rounds[3]], ignore_index = True)\n",
      "/var/folders/sp/zh4t9j256z73nrrj_cn8r_yc0000gp/T/ipykernel_3868/4079259146.py:48: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  f4 = pd.concat([f4,rounds[4]], ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Purdue  vs.  9 TCU\n",
      "Winner: 1 Purdue\n",
      "2 Tennessee  vs.  7 Texas\n",
      "Winner: 2 Tennessee\n",
      "3 Creighton  vs.  11 Oregon\n",
      "Winner: 3 Creighton\n",
      "13 Samford  vs.  5 Gonzaga\n",
      "Winner: 5 Gonzaga\n",
      "1 Purdue  vs.  5 Gonzaga\n",
      "Winner: 1 Purdue\n",
      "2 Tennessee  vs.  3 Creighton\n",
      "Winner: 3 Creighton\n",
      "1 Purdue  vs.  3 Creighton\n",
      "Winner: 1 Purdue\n",
      "________________________________________\n",
      "1 Houston  vs.  16 Longwood\n",
      "Winner: 1 Houston\n",
      "2 Marquette  vs.  15 Western Kentucky\n",
      "Winner: 2 Marquette\n",
      "3 Kentucky  vs.  14 Oakland\n",
      "Winner: 3 Kentucky\n",
      "4 Duke  vs.  13 Vermont\n",
      "Winner: 4 Duke\n",
      "5 Wisconsin  vs.  12 James Madison\n",
      "Winner: 12 James Madison\n",
      "6 Texas Tech  vs.  11 North Carolina St.\n",
      "Winner: 6 Texas Tech\n",
      "7 Florida  vs.  10 Colorado\n",
      "Winner: 10 Colorado\n",
      "8 Nebraska  vs.  9 Texas A&M\n",
      "Winner: 8 Nebraska\n",
      "1 Houston  vs.  8 Nebraska\n",
      "Winner: 1 Houston\n",
      "2 Marquette  vs.  10 Colorado\n",
      "Winner: 2 Marquette\n",
      "3 Kentucky  vs.  6 Texas Tech\n",
      "Winner: 3 Kentucky\n",
      "4 Duke  vs.  12 James Madison\n",
      "Winner: 12 James Madison\n",
      "1 Houston  vs.  12 James Madison\n",
      "Winner: 1 Houston\n",
      "2 Marquette  vs.  3 Kentucky\n",
      "Winner: 3 Kentucky\n",
      "1 Houston  vs.  3 Kentucky\n",
      "Winner: 1 Houston\n",
      "________________________________________\n",
      "1 North Carolina  vs.  16 Wagner\n",
      "Winner: 1 North Carolina\n",
      "2 Arizona  vs.  15 Long Beach St.\n",
      "Winner: 2 Arizona\n",
      "3 Baylor  vs.  14 Colgate\n",
      "Winner: 3 Baylor\n",
      "4 Alabama  vs.  13 College of Charleston\n",
      "Winner: 4 Alabama\n",
      "5 Saint Mary's  vs.  12 Grand Canyon\n",
      "Winner: 12 Grand Canyon\n",
      "6 Clemson  vs.  11 New Mexico\n",
      "Winner: 11 New Mexico\n",
      "7 Dayton  vs.  10 Nevada\n",
      "Winner: 10 Nevada\n",
      "8 Mississippi St.  vs.  9 Michigan St.\n",
      "Winner: 9 Michigan St.\n",
      "1 North Carolina  vs.  9 Michigan St.\n",
      "Winner: 1 North Carolina\n",
      "2 Arizona  vs.  10 Nevada\n",
      "Winner: 2 Arizona\n",
      "3 Baylor  vs.  11 New Mexico\n",
      "Winner: 3 Baylor\n",
      "4 Alabama  vs.  12 Grand Canyon\n",
      "Winner: 4 Alabama\n",
      "1 North Carolina  vs.  4 Alabama\n",
      "Winner: 1 North Carolina\n",
      "2 Arizona  vs.  3 Baylor\n",
      "Winner: 2 Arizona\n",
      "1 North Carolina  vs.  2 Arizona\n",
      "Winner: 2 Arizona\n",
      "________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.DataFrame(columns = df_headers[3:25])\n",
    "y_pred = []\n",
    "matchup_list = []\n",
    "test_regions = [east24_df, midwest24_df, south24_df, west24_df]\n",
    "#print('\\033[1m' + str(learn) + '\\033[0m' + \"\\n\")\n",
    "for x in test_regions: \n",
    "    r64 = pd.concat([r64,x], ignore_index = True)\n",
    "    round64 = x\n",
    "    rounds = [round64, r32, s16, e8, f4]\n",
    "    for r in range (0,len(rounds)-1):\n",
    "        wins = []\n",
    "        y = len(rounds[r])/2\n",
    "        y = int(y)\n",
    "        for x in range(0, y):\n",
    "            h = rounds[r].iloc[x]\n",
    "            l = rounds[r].iloc[-x-1]\n",
    "            holder = pd.DataFrame([get_upset_differences(h,l)], columns = df_headers[3:25])\n",
    "            if holder.iloc[0][\"SEED\"] > 0: \n",
    "                holder = -holder\n",
    "            scaled = scale(holder)\n",
    "            if holder.iloc[0][\"SEED\"] < seed_cutoff_high:\n",
    "                ups = big.predict(scaled)\n",
    "                #print(big.predict_proba(scaled))\n",
    "            elif  holder.iloc[0][\"SEED\"] > seed_cutoff_low:\n",
    "                ups = little.predict(scaled)\n",
    "                #print(little.predict_proba(scaled))\n",
    "            else:\n",
    "                ups = comp.predict(scaled)\n",
    "                #print(comp.predict_proba(scaled))\n",
    "            ups = list(ups)[0]\n",
    "            print(h['SEED'], h['TEAM'], ' vs. ', l['SEED'], l['TEAM'])\n",
    "            if ups == 0: \n",
    "                wins.append(h)\n",
    "                print(\"Winner:\", h['SEED'], h['TEAM'])\n",
    "            if ups == 1:\n",
    "                wins.append(l)\n",
    "                print(\"Winner:\", l['SEED'], l['TEAM'])\n",
    "            test_df = pd.concat([test_df,holder], ignore_index = True)\n",
    "            matchup = h['TEAM'] + ' vs. ' + l['TEAM']\n",
    "            matchup_list.append(matchup)\n",
    "            y_pred.append(ups)\n",
    "        rounds[r+1] = pd.DataFrame(data = wins, columns = df_headers)\n",
    "    print(\"_\" * 40)\n",
    "\n",
    "    r32 = pd.concat([r32,rounds[1]], ignore_index = True)\n",
    "    s16 = pd.concat([s16,rounds[2]], ignore_index = True)\n",
    "    e8 = pd.concat([e8,rounds[3]], ignore_index = True)\n",
    "    f4 = pd.concat([f4,rounds[4]], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL 4 SIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Connecticut vs. 2 Arizona\n",
      "Winner: 1 Connecticut\n",
      "1 Purdue vs. 1 Houston\n",
      "Winner: 1 Houston\n",
      "1 Connecticut vs. 1 Houston\n",
      "Winner: 1 Houston\n"
     ]
    }
   ],
   "source": [
    "final_four = [f4, c2, winner]\n",
    "for r in range (0,len(final_four)-1):\n",
    "    wins = []\n",
    "    y = len(final_four[r])/2\n",
    "    y = int(y)\n",
    "    for x in range(0, y):\n",
    "        h = final_four[r].iloc[x]\n",
    "        l = final_four[r].iloc[-x-1]\n",
    "        holder = pd.DataFrame([get_upset_differences(h,l)], columns = df_headers[3:25]).sort_values(by = \"SEED\")\n",
    "        \n",
    "        scaled = scale(holder)\n",
    "        if holder.iloc[0][\"SEED\"] < seed_cutoff_high:\n",
    "            ups = big.predict(scaled)\n",
    "        elif  holder.iloc[0][\"SEED\"] > seed_cutoff_low:\n",
    "            ups = little.predict(scaled)\n",
    "        else:\n",
    "            ups = comp.predict(scaled)\n",
    "        ups = list(ups)[0]\n",
    "        matchup = str(h['SEED']) + \" \"+ h['TEAM']+  ' vs. '+ str(l['SEED'])+ \" \"+ l['TEAM']\n",
    "        print(matchup)\n",
    "        if ups == 0: \n",
    "            wins.append(h)\n",
    "            print(\"Winner:\", h['SEED'], h['TEAM'])\n",
    "        if ups == 1:\n",
    "            wins.append(l)\n",
    "            print(\"Winner:\", l['SEED'], l['TEAM'])\n",
    "        test_df = pd.concat([test_df,holder], ignore_index = True)\n",
    "        matchup_list.append(matchup)\n",
    "        y_pred.append(ups)\n",
    "    final_four[r+1] = pd.DataFrame(data = wins, columns = df_headers)\n",
    "df_c2 = final_four[1]\n",
    "df_winner = final_four[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['UPSET'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df[\"MATCHUP\"] = matchup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAM</th>\n",
       "      <th>CONF</th>\n",
       "      <th>POSTSEASON</th>\n",
       "      <th>G</th>\n",
       "      <th>W</th>\n",
       "      <th>WIN_PER</th>\n",
       "      <th>ADJOE</th>\n",
       "      <th>ADJDE</th>\n",
       "      <th>BARTHAG</th>\n",
       "      <th>EFG_O</th>\n",
       "      <th>...</th>\n",
       "      <th>FTRD</th>\n",
       "      <th>2P_O</th>\n",
       "      <th>2P_D</th>\n",
       "      <th>3P_O</th>\n",
       "      <th>3P_D</th>\n",
       "      <th>ADJ_T</th>\n",
       "      <th>WAB</th>\n",
       "      <th>SEED</th>\n",
       "      <th>POWER</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Houston</td>\n",
       "      <td>B12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>28</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>121.5</td>\n",
       "      <td>87.8</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>50.1</td>\n",
       "      <td>...</td>\n",
       "      <td>39.6</td>\n",
       "      <td>48.8</td>\n",
       "      <td>44.0</td>\n",
       "      <td>34.8</td>\n",
       "      <td>30.2</td>\n",
       "      <td>63.7</td>\n",
       "      <td>9.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TEAM CONF  POSTSEASON   G   W   WIN_PER  ADJOE  ADJDE  BARTHAG  EFG_O  \\\n",
       "2  Houston  B12         NaN  31  28  0.903226  121.5   87.8   0.9768   50.1   \n",
       "\n",
       "   ...  FTRD  2P_O  2P_D  3P_O  3P_D  ADJ_T  WAB  SEED  POWER  YEAR  \n",
       "2  ...  39.6  48.8  44.0  34.8  30.2   63.7  9.6     1      1  2024  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatStuff(df):\n",
    "    y = df[\"TRAIN\"]\n",
    "    x = scale(df.drop(columns = [\"TRAIN\"]))\n",
    "    return [x,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_r1 = formatStuff(train_w1)\n",
    "train_r2 = formatStuff(train_w2)\n",
    "train_r3 = formatStuff(train_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30833333333333335\n",
      "480 \n",
      "\n",
      "0.38333333333333336\n",
      "120 \n",
      "\n",
      "0.3333333333333333\n",
      "30 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in [train_r1,train_r2,train_r3]:\n",
    "    print(sum(p[1])/len(p[0]))\n",
    "    print(len(p[0]), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DFs\n",
    "x1 = train_r1[0]\n",
    "y1 = list(train_r1[1])\n",
    "\n",
    "x2 = train_r2[0]\n",
    "y2 = list(train_r2[1])\n",
    "\n",
    "x3 = train_r3[0]\n",
    "y3 = list(train_r3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.783\n",
      "Accuracy on training set: 0.933\n",
      "Accuracy on training set: 0.900\n"
     ]
    }
   ],
   "source": [
    "#MLP Classifier\n",
    "\n",
    "iterations = 1000\n",
    "alpha = 4\n",
    " \n",
    "mlp1 = MLPClassifier(max_iter= iterations, alpha=alpha, random_state = 69)\n",
    "mlp1.fit(x1, y1)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp1.score(x1,y1)))\n",
    "\n",
    "mlp2 = MLPClassifier(max_iter= iterations, alpha= alpha, random_state = 52)\n",
    "mlp2.fit(x2, y2)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp2.score(x2, y2)))\n",
    "\n",
    "mlp3 = MLPClassifier(max_iter= iterations, alpha= alpha, random_state = 69)\n",
    "mlp3.fit(x3, y3)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp3.score(x3, y3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.885\n",
      "Accuracy on training set: 0.925\n",
      "Accuracy on training set: 0.967\n"
     ]
    }
   ],
   "source": [
    "#FOREST\n",
    "est = 6\n",
    "depth = 6\n",
    " \n",
    "forest1 = RandomForestClassifier(n_estimators=est, max_depth = depth, random_state = 69)\n",
    "forest1.fit(x1, y1)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    forest1.score(x1, y1)))\n",
    "\n",
    "forest2 = RandomForestClassifier(n_estimators=est, max_depth = depth, random_state = 69)\n",
    "forest2.fit(x2, y2)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    forest2.score(x2, y2)))\n",
    "\n",
    "forest3 = RandomForestClassifier(n_estimators=est, max_depth = depth, random_state = 69)\n",
    "forest3.fit(x3, y3)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest3.score(x3, y3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.823\n",
      "Accuracy on training set: 0.858\n",
      "Accuracy on training set: 0.800\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "svc1 = SVC(random_state = 69, C = 1)\n",
    "svc1.fit(x1, y1)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(svc1.score(x1, y1)))\n",
    "\n",
    "svc2 = SVC(random_state = 69, C = 1)\n",
    "svc2.fit(x2, y2)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(svc2.score(x2, y2)))\n",
    "\n",
    "svc3 = SVC(random_state = 69, C = 1)\n",
    "svc3.fit(x3, y3)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(svc3.score(x3, y3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.783\n",
      "Accuracy on training set: 0.867\n",
      "Accuracy on training set: 0.900\n"
     ]
    }
   ],
   "source": [
    "#Log Regressor\n",
    "\n",
    "clf1 =  LogisticRegression(random_state=69, C =5)\n",
    "clf1.fit(x1, y1)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(clf1.score(x1, y1)))\n",
    "\n",
    "clf2 = LogisticRegression(random_state=69, C = 5)\n",
    "clf2.fit(x2, y2)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(clf2.score(x2, y2)))\n",
    "\n",
    "clf3 = LogisticRegression(random_state=69, C = 3)\n",
    "clf3.fit(x3, y3)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(clf3.score(x3, y3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.692\n",
      "Accuracy on training set: 0.733\n",
      "Accuracy on training set: 0.633\n"
     ]
    }
   ],
   "source": [
    "#K-Nearest\n",
    "neighbors = 5\n",
    "\n",
    "knn1 = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "knn1.fit(x1, y1)\n",
    "knn1_score = knn_big.score(x1, y1)\n",
    "\n",
    "knn2 = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "knn2.fit(x2, y2)\n",
    "knn2_score = knn_little.score(x2, y2)\n",
    "\n",
    "knn3 = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "knn3.fit(x3, y3)\n",
    "knn3_score = knn_comp.score(x3, y3)\n",
    "\n",
    "knn_mean =  sum( [knn1_score, knn2_score, knn3_score])/ len( [knn1_score, knn2_score, knn3_score])\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(knn1_score))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(knn2_score))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(knn3_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.706\n",
      "Accuracy on training set: 0.742\n",
      "Accuracy on training set: 0.867\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "gnb1 = GaussianNB()\n",
    "gnb1.fit(x1, y1)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gnb1.score(x1, y1)))\n",
    "\n",
    "gnb2 = GaussianNB()\n",
    "gnb2.fit(x2, y2)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gnb2.score(x2, y2)))\n",
    "\n",
    "gnb3 = GaussianNB()\n",
    "gnb3.fit(x3, y3)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gnb3.score(x3, y3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.854\n",
      "Accuracy on training set: 0.900\n",
      "Accuracy on training set: 0.867\n"
     ]
    }
   ],
   "source": [
    "# Decision tree\n",
    "\n",
    "tree_depth = 7\n",
    "\n",
    "DT1 = DecisionTreeClassifier(min_samples_leaf = 5, max_depth = tree_depth)\n",
    "DT1.fit(x1, y1)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(DT1.score(x1, y1)))\n",
    "\n",
    "DT2 = DecisionTreeClassifier(min_samples_leaf = 5, max_depth = tree_depth)\n",
    "DT2.fit(x2, y2)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(DT2.score(x2, y2)))\n",
    "\n",
    "DT3 = DecisionTreeClassifier(min_samples_leaf = 5, max_depth = tree_depth)\n",
    "DT3.fit(x3, y3)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(DT3.score(x3, y3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation: Round Group Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r64_r = pd.DataFrame(columns = df_headers)\n",
    "r32_r = pd.DataFrame(columns = df_headers)\n",
    "s16_r = pd.DataFrame(columns = df_headers)\n",
    "e8_r = pd.DataFrame(columns = df_headers)\n",
    "f4_r = pd.DataFrame(columns = df_headers)\n",
    "c2_r= pd.DataFrame(columns = df_headers)\n",
    "winner_r= pd.DataFrame(columns = df_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINE TUNE [knn, DT, forest, mlp, clf, gnb, svc]\n",
    "b = \"clf\"\n",
    "l = 'clf'\n",
    "c = \"clf\"\n",
    "\n",
    "w1 = globals()[b + '%s' % \"1\"]\n",
    "w2 = globals()[l + '%s' % \"2\"]\n",
    "w3 = globals()[c + '%s' % \"3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(columns = df_headers[3:25])\n",
    "y_pred = []\n",
    "matchup_list = []\n",
    "test_regions = [east24_df, midwest24_df, south24_df, west24_df]\n",
    "#print('\\033[1m' + str(learn) + '\\033[0m' + \"\\n\")\n",
    "for x in test_regions: \n",
    "    r64_r = r64_r.append(x)\n",
    "    round64 = x\n",
    "    rounds = [round64, r32_r, s16_r, e8_r, f4_r]\n",
    "    for r in range (0,len(rounds)-1):\n",
    "        wins = []\n",
    "        y = len(rounds[r])/2\n",
    "        y = int(y)\n",
    "        for x in range(0, y):\n",
    "            h = rounds[r].iloc[x]\n",
    "            l = rounds[r].iloc[-x-1]\n",
    "            holder = pd.DataFrame([get_upset_differences(h,l)], columns = df_headers[3:25])\n",
    "            if holder.iloc[0][\"SEED\"] > 0: \n",
    "                holder = -holder\n",
    "            scaled = scale(holder)\n",
    "            if r < 2:\n",
    "                ups = w1.predict(scaled)\n",
    "            else:\n",
    "                ups = w2.predict(scaled)\n",
    "            ups = list(ups)[0]\n",
    "            print(h['SEED'], h['TEAM'], ' vs. ', l['SEED'], l['TEAM'])\n",
    "            if ups == 0: \n",
    "                wins.append(h)\n",
    "                print(\"Winner:\", h['SEED'], h['TEAM'])\n",
    "            if ups == 1:\n",
    "                wins.append(l)\n",
    "                print(\"Winner:\", l['SEED'], l['TEAM'])\n",
    "            test_df = test_df.append(holder)\n",
    "            matchup = h['TEAM'] + ' vs. ' + l['TEAM']\n",
    "            matchup_list.append(matchup)\n",
    "            y_pred.append(ups)\n",
    "        rounds[r+1] = pd.DataFrame(data = wins, columns = df_headers)\n",
    "    print(\"_\" * 40)\n",
    "\n",
    "    r32_r = r32_r.append(rounds[1])\n",
    "    s16_r = s16_r.append(rounds[2])\n",
    "    e8_r = e8_r.append(rounds[3])\n",
    "    f4_r = f4_r.append(rounds[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL 4 SIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_four = [f4_r, c2_r, winner_r]\n",
    "for r in range (0,len(final_four)-1):\n",
    "    wins = []\n",
    "    y = len(final_four[r])/2\n",
    "    y = int(y)\n",
    "    for x in range(0, y):\n",
    "        h = final_four[r].iloc[x]\n",
    "        l = final_four[r].iloc[-x-1]\n",
    "        holder = pd.DataFrame([get_upset_differences(h,l)], columns = df_headers[3:25]).sort_values(by = \"SEED\")\n",
    "        scaled = scale(holder)\n",
    "\n",
    "        ups = w3.predict(scaled)\n",
    "\n",
    "        ups = list(ups)[0]\n",
    "        matchup = str(h['SEED']) + \" \"+ h['TEAM']+  ' vs. '+ str(l['SEED'])+ \" \"+ l['TEAM']\n",
    "        print(matchup)\n",
    "        if ups == 0: \n",
    "            wins.append(h)\n",
    "            print(\"Winner:\", h['SEED'], h['TEAM'])\n",
    "        if ups == 1:\n",
    "            wins.append(l)\n",
    "            print(\"Winner:\", l['SEED'], l['TEAM'])\n",
    "        test_df = test_df.append(holder)\n",
    "        matchup_list.append(matchup)\n",
    "        y_pred.append(ups)\n",
    "    final_four[r+1] = pd.DataFrame(data = wins, columns = df_headers)\n",
    "df_c2 = final_four[1]\n",
    "df_winner = final_four[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['UPSET'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df[\"MATCHUP\"] = matchup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Matchup Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single = region_df([\"Iowa St.\", \"Michigan St.\"],ncaam24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners = [\"knn\", \"DT\", \"forest\", \"mlp\", \"clf\", \"gnb\", \"svc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in learners:\n",
    "    h = single.iloc[0]\n",
    "    l = single.iloc[1]\n",
    "    holder = pd.DataFrame([get_upset_differences(h,l)], columns = df_headers[3:25]).sort_values(by = \"SEED\")\n",
    "    scaled = scale(holder)\n",
    "    \n",
    "    pred = globals()[j + '%s' % \"3\"] #1\n",
    "    ups = pred.predict(scaled)\n",
    "\n",
    "    ups = list(ups)[0]\n",
    "    matchup = str(h['SEED']) + \" \"+ h['TEAM']+  ' vs. '+ str(l['SEED'])+ \" \"+ l['TEAM']\n",
    "    print(matchup)\n",
    "    if ups == 0: \n",
    "        wins.append(h)\n",
    "        print(\"Winner:\", h['SEED'], h['TEAM'])\n",
    "    if ups == 1:\n",
    "        wins.append(l)\n",
    "        print(\"Winner:\", l['SEED'], l['TEAM'])\n",
    "    print(\"_\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in learners:\n",
    "    h = single.iloc[1]\n",
    "    l = single.iloc[0]\n",
    "    holder = pd.DataFrame([get_upset_differences(h,l)], columns = df_headers[3:25]).sort_values(by = \"SEED\")\n",
    "    scaled = scale(holder)\n",
    "    \n",
    "    pred = globals()[j + '%s' % \"_comp\"] #1\n",
    "    ups = pred.predict(scaled)\n",
    "\n",
    "    ups = list(ups)[0]\n",
    "    matchup = str(h['SEED']) + \" \"+ h['TEAM']+  ' vs. '+ str(l['SEED'])+ \" \"+ l['TEAM']\n",
    "    print(matchup)\n",
    "    if ups == 0: \n",
    "        wins.append(h)\n",
    "        print(\"Winner:\", h['SEED'], h['TEAM'])\n",
    "    if ups == 1:\n",
    "        wins.append(l)\n",
    "        print(\"Winner:\", l['SEED'], l['TEAM'])\n",
    "    print(\"_\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conference Tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bye_split(team_cnt):\n",
    "    x = 0\n",
    "    while 2**x < team_cnt:\n",
    "        binary_rnd = 2**x \n",
    "        x += 1\n",
    "    \n",
    "    second_round = binary_rnd + (binary_rnd - team_cnt)\n",
    "    \n",
    "    return second_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_rnd_bye(df):\n",
    "    teams = bye_split(len(df))\n",
    "    bye_teams = df.iloc[:teams]\n",
    "    first_round = df.iloc[teams:]\n",
    "    \n",
    "    return bye_teams, first_round\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_east = ['Connecticut',\n",
    " 'Creighton',\n",
    " 'Marquette',\n",
    " 'Seton Hall',\n",
    " \"St. John's\",\n",
    " 'Villanova',\n",
    " 'Providence',\n",
    " 'Butler',\n",
    " 'Xavier',\n",
    " 'Georgetown',\n",
    " 'DePaul']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_df(x,y):\n",
    "    v = pd.DataFrame(x)\n",
    "    v = v.rename(columns={0: \"TEAM\"})\n",
    "    v = v.merge(y, on = 'TEAM', how='left')\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "be_tourn = region_df(big_east, ncaam24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "be_tourn.SEED = be_tourn.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2, r1 = first_rnd_bye(be_tourn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(r1) > 0:\n",
    "    for x in range(int(len(be24[1])/2)):\n",
    "        h = r1.iloc[x]\n",
    "        l = r1.iloc[(-x-1)]\n",
    "        holder = pd.DataFrame([get_upset_differences(h,l)], columns = df_headers[3:25])\n",
    "        scaled = scale(holder)\n",
    "        \n",
    "\n",
    "        \n",
    "        pred = forest_comp\n",
    "        ups = pred.predict(scaled)\n",
    "\n",
    "        ups = list(ups)[0]\n",
    "        matchup = str(h['SEED']) + \" \"+ h['TEAM']+  ' vs. '+ str(l['SEED'])+ \" \"+ l['TEAM']\n",
    "        print(matchup)\n",
    "        if ups == 0: \n",
    "            r2 = r2.append(h, ignore_index=True)\n",
    "            print(\"Winner:\", h['SEED'], h['TEAM'])\n",
    "        if ups == 1:\n",
    "            r2 = r2.append(l, ignore_index=True)\n",
    "            print(\"Winner:\", l['SEED'], l['TEAM'])\n",
    "        print(\"_\"*40)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 2\n",
    "while len(globals()[\"r\" + '%s' % str(r)]) != 1:\n",
    "    curr_round = globals()[\"r\" + '%s' % str(r)]\n",
    "    \n",
    "    globals()[\"r\" + '%s' % str((r+1))] = pd.DataFrame(columns = df_headers)\n",
    "    for x in range(int(len(curr_round)/2)):\n",
    "        h = curr_round.iloc[x]\n",
    "        l = curr_round.iloc[(-x-1)]\n",
    "        holder = pd.DataFrame([get_upset_differences(h,l)], columns = df_headers[3:25])\n",
    "        scaled = scale(holder)\n",
    "        \n",
    "        if holder.iloc[0][\"SEED\"] < seed_cutoff_high:\n",
    "            pred = mlp_big\n",
    "            #print(big.predict_proba(scaled))\n",
    "        elif  holder.iloc[0][\"SEED\"] > seed_cutoff_low:\n",
    "            pred = mlp_little\n",
    "            #print(little.predict_proba(scaled))\n",
    "        else:\n",
    "            pred = mlp_comp\n",
    "            #print(comp.predict_proba(scaled))\n",
    "        \n",
    "        ups = pred.predict(scaled)\n",
    "\n",
    "        ups = list(ups)[0]\n",
    "        matchup = str(h['SEED']) + \" \"+ h['TEAM']+  ' vs. '+ str(l['SEED'])+ \" \"+ l['TEAM']\n",
    "        print(matchup)\n",
    "        if ups == 0: \n",
    "            globals()[\"r\" + '%s' % str((r+1))] = globals()[\"r\" + '%s' % str((r+1))].append(h, ignore_index=True)\n",
    "            print(\"Winner:\", h['SEED'], h['TEAM'])\n",
    "        if ups == 1:\n",
    "            globals()[\"r\" + '%s' % str((r+1))] = globals()[\"r\" + '%s' % str((r+1))].append(l, ignore_index=True)\n",
    "            print(\"Winner:\", l['SEED'], l['TEAM'])\n",
    "        print(\"_\"*40)\n",
    "    \n",
    "    \n",
    "    r += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners = [\"knn\", \"DT\", \"forest\", \"mlp\", \"clf\", \"gnb\", \"svc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COEF Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lasso\n",
    "\n",
    "X = train_big[0]\n",
    "Y = train_big[1]\n",
    "\n",
    "x = train_little[0]\n",
    "y = train_little[1]\n",
    "\n",
    "train_x = train_comp[0]\n",
    "train_y = train_comp[1]\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso_big = Lasso(alpha=0.005)\n",
    "lasso_big.fit(X,Y)\n",
    "\n",
    "lasso_little = Lasso(alpha=0.005)\n",
    "lasso_little.fit(x,y)\n",
    "\n",
    "lasso_comp = Lasso(alpha=0.005)\n",
    "lasso_comp.fit(train_x,train_y)\n",
    "\n",
    "pd.DataFrame([lasso_big.coef_, lasso_little.coef_, lasso_comp.coef_], columns  = new_columns[3:25], index=[\"big_upset\", \"little_upset\", \"competative\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Final Fours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scen = []\n",
    "learning = []\n",
    "upset_count = []\n",
    "winners= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f in range(7):\n",
    "    for g in range(7):\n",
    "\n",
    "        r64 = pd.DataFrame(columns = df_headers)\n",
    "        r32 = pd.DataFrame(columns = df_headers)\n",
    "        s16 = pd.DataFrame(columns = df_headers)\n",
    "        e8 = pd.DataFrame(columns = df_headers)\n",
    "        f4 = pd.DataFrame(columns = df_headers)\n",
    "        c2= pd.DataFrame(columns = df_headers)\n",
    "        winner= pd.DataFrame(columns = df_headers)\n",
    "\n",
    "        #FINE TUNE [knn, DT, forest, mlp, clf, gnb, svc]\n",
    "        learners = [\"knn\", \"DT\", \"forest\", \"mlp\", \"clf\", \"gnb\", \"svc\"]\n",
    "        b = learners[f]\n",
    "        l = learners[g]\n",
    "\n",
    "        w1 = globals()[b + '%s' % \"1\"]\n",
    "        w2 = globals()[l + '%s' % \"2\"]\n",
    "\n",
    "        test_df = pd.DataFrame(columns = df_headers[3:25])\n",
    "        y_pred = []\n",
    "        matchup_list = []\n",
    "        test_regions = [east24_df, midwest24_df, south24_df, west24_df]\n",
    "\n",
    "        for x in test_regions: \n",
    "            r64 = r64.append(x)\n",
    "            round64 = x\n",
    "            rounds = [round64, r32, s16, e8, f4]\n",
    "            for r in range (0,len(rounds)-1):\n",
    "                wins = []\n",
    "                y = len(rounds[r])/2\n",
    "                y = int(y)\n",
    "                for x in range(0, y):\n",
    "                    base_count = 0\n",
    "\n",
    "                    h = rounds[r].iloc[x]\n",
    "                    l = rounds[r].iloc[-x-1]\n",
    "                    holder = pd.DataFrame([get_upset_differences(h,l)], columns = df_headers[3:25])\n",
    "                    if holder.iloc[0][\"SEED\"] > 0: \n",
    "                        holder = -holder\n",
    "                    scaled = scale(holder)\n",
    "                    if r < 2:\n",
    "                        ups = w1.predict(scaled)\n",
    "                    else:\n",
    "                        ups = w2.predict(scaled)\n",
    "#                    if holder.iloc[0][\"SEED\"] < seed_cutoff_high:\n",
    "#                       ups = big.predict(scaled)\n",
    "#                        #print(big.predict_proba(scaled))\n",
    "#                    elif  holder.iloc[0][\"SEED\"] > seed_cutoff_low:\n",
    "#                        ups = little.predict(scaled)\n",
    "#                        #print(little.predict_proba(scaled))\n",
    "#                    else:\n",
    "#                        ups = comp.predict(scaled)\n",
    "#                        #print(comp.predict_proba(scaled))\n",
    "                    \n",
    "                    ups = list(ups)[0]\n",
    "                    if ups == 0: \n",
    "                        wins.append(h)\n",
    "                    if ups == 1:\n",
    "                        wins.append(l)\n",
    "                        base_count += 1\n",
    "                    test_df = test_df.append(holder)\n",
    "                    matchup = h['TEAM'] + ' vs. ' + l['TEAM']\n",
    "                    matchup_list.append(matchup)\n",
    "                    y_pred.append(ups)\n",
    "\n",
    "                    upset_count.append(base_count)\n",
    "                rounds[r+1] = pd.DataFrame(data = wins, columns = df_headers)\n",
    "                \n",
    "                \n",
    "            r32 = r32.append(rounds[1])\n",
    "            s16 = s16.append(rounds[2])\n",
    "            e8 = e8.append(rounds[3])\n",
    "            f4 = f4.append(rounds[4])\n",
    "            winners.append(rounds[1:])\n",
    "        \n",
    "        learning.append(learners[f] + \" \" + learners[g])\n",
    "        all_scen.append(list(f4.TEAM))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Four Proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(upset_count)/15/49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scen1 = []\n",
    "for j in all_scen:\n",
    "    all_scen1.append(list(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = pd.DataFrame(all_scen1, columns = [\"East\", \"Midwest\", \"West\", \"South\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios.index = learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_east = max(scenarios.groupby([\"East\"]).count()[\"South\"])\n",
    "scenarios.groupby([\"East\"]).count()[\"South\"].sort_values(ascending = False)/max_east*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_south = max(scenarios.groupby([\"South\"]).count()[\"East\"])\n",
    "scenarios.groupby([\"South\"]).count()[\"East\"].sort_values(ascending = False)/max_south*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_midwest = max(scenarios.groupby([\"Midwest\"]).count()[\"East\"])\n",
    "scenarios.groupby([\"Midwest\"]).count()[\"East\"].sort_values(ascending = False)/max_midwest*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_west = max(scenarios.groupby([\"West\"]).count()[\"East\"])\n",
    "scenarios.groupby([\"West\"]).count()[\"East\"].sort_values(ascending = False)/max_west*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios.to_csv('Sims/sims24.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(winners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r32all = pd.DataFrame(columns = ['TEAM','SEED','CONF'])\n",
    "s16all = pd.DataFrame(columns = ['TEAM','SEED','CONF'])\n",
    "e8all = pd.DataFrame(columns = ['TEAM','SEED','CONF'])\n",
    "f4all = pd.DataFrame(columns = ['TEAM','SEED','CONF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in winners:\n",
    "    r32all = r32all.append(x[0][['TEAM','SEED','CONF']])\n",
    "    s16all = s16all.append(x[1][['TEAM','SEED','CONF']])\n",
    "    e8all = e8all.append(x[2][['TEAM','SEED','CONF']])\n",
    "    f4all = f4all.append(x[3][['TEAM','SEED','CONF']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_setter(number):\n",
    "    regions = [\"east\", \"midwest\", \"south\", \"west\"]\n",
    "    n = int(number/ 196)\n",
    "    \n",
    "    region_col = []\n",
    "    for i in regions:\n",
    "        x1 = n * [i]\n",
    "        region_col.extend(x1)\n",
    "    return region_col * 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learner_setter(number):\n",
    "    scens = list(scenarios.index)\n",
    "    n = int(number/ 49)\n",
    "    \n",
    "    learner_col = []\n",
    "    for i in scens:\n",
    "        x1 = n * [i]\n",
    "        learner_col.extend(x1)\n",
    "    return learner_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r32all[\"REGION\"] = region_setter(len(r32all))\n",
    "r32all[\"LEARNER\"] = learner_setter(len(r32all))\n",
    "\n",
    "s16all[\"REGION\"] = region_setter(len(s16all))\n",
    "s16all[\"LEARNER\"] = learner_setter(len(s16all))\n",
    "\n",
    "e8all[\"REGION\"] = region_setter(len(e8all))\n",
    "e8all[\"LEARNER\"] = learner_setter(len(e8all))\n",
    "\n",
    "[\"REGION\"] = region_setter(len(f4all))\n",
    "f4all[\"LEARNER\"] = learner_setter(len(f4all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f4 = max(f4all.groupby([\"TEAM\"]).count()[\"LEARNER\"])\n",
    "f4all.groupby([\"TEAM\", \"REGION\"]).count()[\"LEARNER\"].sort_values(ascending = False)/max_f4*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_SEED32 = max(r32all.groupby([\"SEED\"]).count()[\"LEARNER\"])\n",
    "r32all.groupby([\"SEED\"]).count()[\"LEARNER\"].sort_values(ascending = False)/max_SEED32*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r32all[r32all['SEED'] >= 11].groupby(['TEAM',\"SEED\",\"REGION\"]).count()[\"LEARNER\"].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s16all[s16all['SEED'] >= 6].groupby(['TEAM',\"SEED\",\"REGION\"]).count()[\"LEARNER\"].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f4all.groupby(['TEAM',\"SEED\",\"REGION\"]).count()[\"LEARNER\"].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r32all[r32all['REGION'] == \"midwest\"].groupby(['TEAM',\"SEED\",\"REGION\"]).count()[\"LEARNER\"].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios1 = scenarios[scenarios[\"East\"] == \"Tennessee\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "east_seed = []\n",
    "for x in scenarios['East']:\n",
    "    east_seed.append(east23.index(x)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "south_seed = []\n",
    "for x in scenarios['South']:\n",
    "    south_seed.append(south23.index(x)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "west_seed = []\n",
    "for x in scenarios['West']:\n",
    "    west_seed.append(west23.index(x)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midwest_seed = []\n",
    "for x in scenarios['Midwest']:\n",
    "    midwest_seed.append(midwest23.index(x)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios['East_seed'] = east_seed\n",
    "scenarios['west_seed'] = west_seed\n",
    "scenarios['midwest_seed'] = midwest_seed\n",
    "\n",
    "scenarios['south_seed'] = south_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios['seed_sum'] = scenarios['East_seed'] + scenarios['south_seed'] + scenarios['west_seed'] + scenarios['midwest_seed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_upset_sum = []\n",
    "for i in range (0, len(upset_count), 15):\n",
    "    x = i\n",
    "    region_upset_sum.append(upset_count[x:x+15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(region_upset_sum[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_split = []\n",
    "for y in region_upset_sum:\n",
    "    ind_list = []\n",
    "    ind_list.append(sum(y[0:7]))\n",
    "    ind_list.append(sum(y[8:11]))\n",
    "    ind_list.append(sum(y[12:13]))\n",
    "    ind_list.append(y[14])\n",
    "    leg_split.append(ind_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upset_col = [\n",
    "    \"east64\",\"east32\",\"east16\",\"east8\", \n",
    "    \"south64\",\"south32\",\"south16\",\"south8\",\n",
    "    'midwest64','midwest32','midwest16','midwest8',\n",
    "    'west64','west32','west16','west8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ncaam[ncaam['POSTSEASON'] in  ['2ND', 'F4', 'Champions'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y1)/len(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
